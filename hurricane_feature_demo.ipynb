{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBx3oiQvMOAL"
      },
      "source": [
        "# GraphCast SAE Demo\n",
        "\n",
        "This notebook demonstrates loading GraphCast and extracting internal activations."
      ],
      "id": "oBx3oiQvMOAL"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_tZV8gNwMOAN",
        "outputId": "9a4596df-4029-4832-a70c-61845f642e27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/theodoremacmillan/graphcast.git@sae-hooks\n",
            "  Cloning https://github.com/theodoremacmillan/graphcast.git (to revision sae-hooks) to /tmp/pip-req-build-as2radib\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/theodoremacmillan/graphcast.git /tmp/pip-req-build-as2radib\n",
            "  Running command git checkout -b sae-hooks --track origin/sae-hooks\n",
            "  Switched to a new branch 'sae-hooks'\n",
            "  Branch 'sae-hooks' set up to track remote branch 'sae-hooks' from 'origin'.\n",
            "  Resolved https://github.com/theodoremacmillan/graphcast.git to commit 39d1de436148c5658726b5a092bd2d7ef8701f2a\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cartopy (from graphcast==0.2.0.dev0)\n",
            "  Downloading cartopy-0.25.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: chex in /usr/local/lib/python3.11/dist-packages (from graphcast==0.2.0.dev0) (0.1.89)\n",
            "Collecting colabtools (from graphcast==0.2.0.dev0)\n",
            "  Downloading colabtools-0.0.1-py3-none-any.whl.metadata (511 bytes)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.11/dist-packages (from graphcast==0.2.0.dev0) (2024.12.1)\n",
            "Collecting dinosaur-dycore (from graphcast==0.2.0.dev0)\n",
            "  Downloading dinosaur_dycore-1.2.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting dm-haiku (from graphcast==0.2.0.dev0)\n",
            "  Downloading dm_haiku-0.0.15-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from graphcast==0.2.0.dev0) (0.1.9)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from graphcast==0.2.0.dev0) (0.5.2)\n",
            "Collecting jraph (from graphcast==0.2.0.dev0)\n",
            "  Downloading jraph-0.0.6.dev0-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from graphcast==0.2.0.dev0) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from graphcast==0.2.0.dev0) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from graphcast==0.2.0.dev0) (2.2.2)\n",
            "Collecting rtree (from graphcast==0.2.0.dev0)\n",
            "  Downloading rtree-1.4.1-py3-none-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from graphcast==0.2.0.dev0) (1.15.3)\n",
            "Collecting trimesh (from graphcast==0.2.0.dev0)\n",
            "  Downloading trimesh-4.10.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from graphcast==0.2.0.dev0) (4.14.1)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.11/dist-packages (from graphcast==0.2.0.dev0) (2025.3.1)\n",
            "Collecting xarray_tensorstore (from graphcast==0.2.0.dev0)\n",
            "  Downloading xarray_tensorstore-0.3.0-py3-none-any.whl.metadata (600 bytes)\n",
            "Requirement already satisfied: shapely>=2.0 in /usr/local/lib/python3.11/dist-packages (from cartopy->graphcast==0.2.0.dev0) (2.1.1)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from cartopy->graphcast==0.2.0.dev0) (24.2)\n",
            "Requirement already satisfied: pyshp>=2.3 in /usr/local/lib/python3.11/dist-packages (from cartopy->graphcast==0.2.0.dev0) (2.3.1)\n",
            "Requirement already satisfied: pyproj>=3.3.1 in /usr/local/lib/python3.11/dist-packages (from cartopy->graphcast==0.2.0.dev0) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->graphcast==0.2.0.dev0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->graphcast==0.2.0.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->graphcast==0.2.0.dev0) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->graphcast==0.2.0.dev0) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->graphcast==0.2.0.dev0) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->graphcast==0.2.0.dev0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->graphcast==0.2.0.dev0) (2.9.0.post0)\n",
            "Requirement already satisfied: absl-py>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex->graphcast==0.2.0.dev0) (1.4.0)\n",
            "Requirement already satisfied: jaxlib>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from chex->graphcast==0.2.0.dev0) (0.5.1)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex->graphcast==0.2.0.dev0) (0.12.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax->graphcast==0.2.0.dev0) (0.4.1)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax->graphcast==0.2.0.dev0) (3.4.0)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.11/dist-packages (from dask->graphcast==0.2.0.dev0) (8.2.1)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dask->graphcast==0.2.0.dev0) (3.1.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.11/dist-packages (from dask->graphcast==0.2.0.dev0) (2025.3.2)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask->graphcast==0.2.0.dev0) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from dask->graphcast==0.2.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: importlib_metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask->graphcast==0.2.0.dev0) (8.7.0)\n",
            "Collecting pint (from dinosaur-dycore->graphcast==0.2.0.dev0)\n",
            "  Downloading pint-0.25.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from dinosaur-dycore->graphcast==0.2.0.dev0) (1.6.1)\n",
            "Collecting tree-math (from dinosaur-dycore->graphcast==0.2.0.dev0)\n",
            "  Downloading tree_math-0.2.1-py3-none-any.whl.metadata (477 bytes)\n",
            "Collecting jmp>=0.0.2 (from dm-haiku->graphcast==0.2.0.dev0)\n",
            "  Downloading jmp-0.0.4-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from dm-haiku->graphcast==0.2.0.dev0) (0.9.0)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->graphcast==0.2.0.dev0) (25.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from dm-tree->graphcast==0.2.0.dev0) (1.17.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->graphcast==0.2.0.dev0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->graphcast==0.2.0.dev0) (2025.2)\n",
            "Collecting zarr (from xarray_tensorstore->graphcast==0.2.0.dev0)\n",
            "  Downloading zarr-3.1.5-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.11/dist-packages (from xarray_tensorstore->graphcast==0.2.0.dev0) (0.1.74)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.13.0->dask->graphcast==0.2.0.dev0) (3.23.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.11/dist-packages (from partd>=1.4.0->dask->graphcast==0.2.0.dev0) (1.0.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from pyproj>=3.3.1->cartopy->graphcast==0.2.0.dev0) (2025.7.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->graphcast==0.2.0.dev0) (1.17.0)\n",
            "Collecting flexcache>=0.3 (from pint->dinosaur-dycore->graphcast==0.2.0.dev0)\n",
            "  Downloading flexcache-0.3-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting flexparser>=0.4 (from pint->dinosaur-dycore->graphcast==0.2.0.dev0)\n",
            "  Downloading flexparser-0.4-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: platformdirs>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pint->dinosaur-dycore->graphcast==0.2.0.dev0) (4.3.8)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->dinosaur-dycore->graphcast==0.2.0.dev0) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->dinosaur-dycore->graphcast==0.2.0.dev0) (3.6.0)\n",
            "Collecting donfig>=0.8 (from zarr->xarray_tensorstore->graphcast==0.2.0.dev0)\n",
            "  Downloading donfig-0.8.1.post1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: google-crc32c>=1.5 in /usr/local/lib/python3.11/dist-packages (from zarr->xarray_tensorstore->graphcast==0.2.0.dev0) (1.7.1)\n",
            "Collecting numcodecs>=0.14 (from zarr->xarray_tensorstore->graphcast==0.2.0.dev0)\n",
            "  Downloading numcodecs-0.16.5-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading cartopy-0.25.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colabtools-0.0.1-py3-none-any.whl (14 kB)\n",
            "Downloading dinosaur_dycore-1.2.1-py3-none-any.whl (172 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.3/172.3 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dm_haiku-0.0.15-py3-none-any.whl (374 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.1/374.1 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jraph-0.0.6.dev0-py3-none-any.whl (90 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rtree-1.4.1-py3-none-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (507 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.6/507.6 kB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trimesh-4.10.1-py3-none-any.whl (737 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m737.0/737.0 kB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xarray_tensorstore-0.3.0-py3-none-any.whl (10 kB)\n",
            "Downloading jmp-0.0.4-py3-none-any.whl (18 kB)\n",
            "Downloading pint-0.25.2-py3-none-any.whl (306 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.8/306.8 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tree_math-0.2.1-py3-none-any.whl (21 kB)\n",
            "Downloading zarr-3.1.5-py3-none-any.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.1/284.1 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading donfig-0.8.1.post1-py3-none-any.whl (21 kB)\n",
            "Downloading flexcache-0.3-py3-none-any.whl (13 kB)\n",
            "Downloading flexparser-0.4-py3-none-any.whl (27 kB)\n",
            "Downloading numcodecs-0.16.5-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: graphcast\n",
            "  Building wheel for graphcast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for graphcast: filename=graphcast-0.2.0.dev0-py3-none-any.whl size=139604 sha256=f010f75bdfe2c3ca1db0bb04d21ac21c4a7dcbb078cabc5d65a2bfb3e849733a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-lkpshj0l/wheels/f9/35/90/510d678aff401d43c3c7178992add1d3d2b1dfbda05c3ce730\n",
            "Successfully built graphcast\n",
            "Installing collected packages: colabtools, trimesh, rtree, numcodecs, jmp, flexparser, flexcache, donfig, zarr, pint, dm-haiku, cartopy, xarray_tensorstore, tree-math, jraph, dinosaur-dycore, graphcast\n",
            "Successfully installed cartopy-0.25.0 colabtools-0.0.1 dinosaur-dycore-1.2.1 dm-haiku-0.0.15 donfig-0.8.1.post1 flexcache-0.3 flexparser-0.4 graphcast-0.2.0.dev0 jmp-0.0.4 jraph-0.0.6.dev0 numcodecs-0.16.5 pint-0.25.2 rtree-1.4.1 tree-math-0.2.1 trimesh-4.10.1 xarray_tensorstore-0.3.0 zarr-3.1.5\n"
          ]
        }
      ],
      "source": [
        "%pip install git+https://github.com/theodoremacmillan/graphcast.git@sae-hooks\n"
      ],
      "id": "_tZV8gNwMOAN"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Workaround for cartopy crashes\n",
        "\n",
        "# Workaround for cartopy crashes due to the shapely installed by default in\n",
        "# google colab kernel (https://github.com/anitagraser/movingpandas/issues/81):\n",
        "!pip uninstall -y shapely\n",
        "!pip install shapely --no-binary shapely"
      ],
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "wiMX8gHfZ5eO",
        "outputId": "b87a03f2-b023-4ed0-9123-255a105ebfbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "wiMX8gHfZ5eO",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: shapely 2.1.1\n",
            "Uninstalling shapely-2.1.1:\n",
            "  Successfully uninstalled shapely-2.1.1\n",
            "Collecting shapely\n",
            "  Downloading shapely-2.1.2.tar.gz (315 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.5/315.5 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.11/dist-packages (from shapely) (2.0.2)\n",
            "Building wheels for collected packages: shapely\n",
            "  Building wheel for shapely (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for shapely: filename=shapely-2.1.2-cp311-cp311-linux_x86_64.whl size=1207446 sha256=29c667a1fc19cc5deeabfead39c38b220a1cf17c4aae21131e3c70eece9f24fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/a4/43/7be70b9a914836f51744c5e6e2408c9b4d0c3bcb2033d394e0\n",
            "Successfully built shapely\n",
            "Installing collected packages: shapely\n",
            "Successfully installed shapely-2.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Imports\n",
        "\n",
        "import dataclasses\n",
        "import datetime\n",
        "import functools\n",
        "import math\n",
        "import re\n",
        "from typing import Optional\n",
        "\n",
        "import cartopy.crs as ccrs\n",
        "from google.cloud import storage\n",
        "from graphcast import autoregressive\n",
        "from graphcast import casting\n",
        "from graphcast import checkpoint\n",
        "from graphcast import data_utils\n",
        "from graphcast import graphcast\n",
        "from graphcast import normalization\n",
        "from graphcast import rollout\n",
        "from graphcast import xarray_jax\n",
        "from graphcast import xarray_tree\n",
        "from IPython.display import HTML\n",
        "import ipywidgets as widgets\n",
        "import haiku as hk\n",
        "import jax\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "import numpy as np\n",
        "import xarray\n",
        "\n",
        "\n",
        "def parse_file_parts(file_name):\n",
        "  return dict(part.split(\"-\", 1) for part in file_name.split(\"_\"))\n"
      ],
      "metadata": {
        "id": "itwtuUbXaWJF"
      },
      "id": "itwtuUbXaWJF",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Authenticate with Google Cloud Storage\n",
        "\n",
        "gcs_client = storage.Client.create_anonymous_client()\n",
        "gcs_bucket = gcs_client.get_bucket(\"dm_graphcast\")\n",
        "dir_prefix = \"graphcast/\""
      ],
      "metadata": {
        "id": "NTjuMJH2aTmf"
      },
      "id": "NTjuMJH2aTmf",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import xarray as xr\n",
        "import gcsfs\n",
        "\n",
        "\n",
        "def load_era5_into_memory(\n",
        "    start: str,\n",
        "    end: str,\n",
        "    zarr_path: str = \"gs://weatherbench2/datasets/era5/1959-2022-full_37-6h-0p25deg_derived.zarr\",\n",
        "    vars_keep=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Load a slice of ERA5 data fully into memory.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    start, end : str\n",
        "        Date range (YYYY-MM-DD)\n",
        "    zarr_path : str\n",
        "        GCS or local Zarr path\n",
        "    vars_keep : list[str] | None\n",
        "        Variables to keep (None = all)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    xarray.Dataset\n",
        "        Fully-loaded dataset in memory\n",
        "    \"\"\"\n",
        "    if vars_keep is None:\n",
        "      vars_keep = [\n",
        "          \"geopotential\",\n",
        "          \"specific_humidity\",\n",
        "          \"temperature\",\n",
        "          \"u_component_of_wind\",\n",
        "          \"v_component_of_wind\",\n",
        "          \"vertical_velocity\",\n",
        "          \"2m_temperature\",\n",
        "          \"10m_u_component_of_wind\",\n",
        "          \"10m_v_component_of_wind\",\n",
        "          \"mean_sea_level_pressure\",\n",
        "          \"total_precipitation_6hr\",\n",
        "          \"toa_incident_solar_radiation\",\n",
        "          \"geopotential_at_surface\",\n",
        "          \"land_sea_mask\"\n",
        "      ]\n",
        "\n",
        "    start = np.datetime64(start)\n",
        "    end = np.datetime64(end)\n",
        "\n",
        "    # --- Open Zarr store ---\n",
        "    if zarr_path.startswith(\"gs://\"):\n",
        "        fs = gcsfs.GCSFileSystem(token=\"anon\")\n",
        "        store = fs.get_mapper(zarr_path[5:])\n",
        "        ds = xr.open_zarr(store, consolidated=True)\n",
        "    else:\n",
        "        ds = xr.open_zarr(zarr_path, consolidated=True)\n",
        "\n",
        "    # --- Normalize coords ---\n",
        "    rename = {}\n",
        "    if \"latitude\" in ds.coords:\n",
        "        rename[\"latitude\"] = \"lat\"\n",
        "    if \"longitude\" in ds.coords:\n",
        "        rename[\"longitude\"] = \"lon\"\n",
        "    if rename:\n",
        "        ds = ds.rename(rename)\n",
        "\n",
        "    if ds.lat[0] > ds.lat[-1]:\n",
        "        ds = ds.reindex(lat=ds.lat[::-1])\n",
        "\n",
        "    # --- Time slice ---\n",
        "    ds = ds.sel(time=slice(start, end))\n",
        "\n",
        "    # --- Variable selection ---\n",
        "    if vars_keep is not None:\n",
        "        ds = ds[[v for v in vars_keep if v in ds.data_vars]]\n",
        "\n",
        "    # --- LOAD EVERYTHING INTO MEMORY ---\n",
        "    ds = ds.load()\n",
        "\n",
        "    return ds"
      ],
      "metadata": {
        "id": "wX8fU5VsbFL2"
      },
      "id": "wX8fU5VsbFL2",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_era5_into_memory(\n",
        "    start=\"2020-01-01\",\n",
        "    end=\"2020-01-02\"\n",
        ")"
      ],
      "metadata": {
        "id": "CZd38Fs0bYwO"
      },
      "id": "CZd38Fs0bYwO",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "\n",
        "def write_daily_era5_files(ds: xr.Dataset, out_dir: str):\n",
        "    \"\"\"\n",
        "    Write an in-memory ERA5 Dataset to daily NetCDF files\n",
        "    compatible with three_step_window().\n",
        "\n",
        "    Assumes:\n",
        "      - ds has a 'time' coordinate of type datetime64\n",
        "      - 6-hourly (or finer) resolution\n",
        "    \"\"\"\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # Group by day\n",
        "    for day, ds_day in ds.groupby(\"time.date\"):\n",
        "        day_str = np.datetime_as_string(np.datetime64(day), unit=\"D\")\n",
        "        out_path = os.path.join(out_dir, f\"era5_{day_str}.nc\")\n",
        "\n",
        "        # Preserve original encoding as much as possible\n",
        "        ds_day.to_netcdf(out_path)\n",
        "\n",
        "        print(f\"[WRITE] {out_path}\")\n"
      ],
      "metadata": {
        "id": "_OBGBhhynHPL"
      },
      "id": "_OBGBhhynHPL",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_daily_era5_files(\n",
        "    ds,\n",
        "    out_dir = '/content/era5_daily_nc'\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CfbjBt4AeMx2",
        "outputId": "e3d82a2d-af5d-45b4-ce4d-8453fad6f908",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "CfbjBt4AeMx2",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WRITE] /content/era5_daily_nc/era5_2020-01-01.nc\n",
            "[WRITE] /content/era5_daily_nc/era5_2020-01-02.nc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next step is running GraphCast and capturing its internal activations at layer 8"
      ],
      "metadata": {
        "id": "wV_ue8_-gIVB"
      },
      "id": "wV_ue8_-gIVB"
    },
    {
      "cell_type": "code",
      "source": [
        "import dataclasses\n",
        "import functools\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "import jax\n",
        "import haiku as hk\n",
        "\n",
        "from graphcast import (\n",
        "    autoregressive,\n",
        "    casting,\n",
        "    checkpoint,\n",
        "    data_utils,\n",
        "    graphcast,\n",
        "    normalization,\n",
        "    rollout,\n",
        "    xarray_jax,\n",
        "    xarray_tree,\n",
        ")\n",
        "\n",
        "from graphcast.deep_typed_graph_net import get_activation_manager\n",
        "from google.cloud import storage\n"
      ],
      "metadata": {
        "id": "l9w0CKhMgOs9"
      },
      "id": "l9w0CKhMgOs9",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# FULL GraphCast activation pipeline — faithful to original\n",
        "# ============================================================\n",
        "\n",
        "import os, glob, dataclasses, functools, time\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "import jax, haiku as hk\n",
        "from google.cloud import storage\n",
        "\n",
        "from graphcast import (\n",
        "    autoregressive,\n",
        "    casting,\n",
        "    checkpoint,\n",
        "    data_utils,\n",
        "    graphcast,\n",
        "    normalization,\n",
        "    rollout,\n",
        "    xarray_jax,\n",
        "    xarray_tree,\n",
        ")\n",
        "from graphcast.deep_typed_graph_net import get_activation_manager\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# USER INPUTS (YOU SET THESE)\n",
        "# ============================================================\n",
        "\n",
        "data_dir = \"/content/era5_daily_nc\"        # contains era5_YYYY-MM-DD.nc\n",
        "acts_dir = \"/content/graphcast_acts\"\n",
        "os.makedirs(acts_dir, exist_ok=True)\n",
        "\n",
        "centers = np.arange(\n",
        "    np.datetime64(\"2020-01-01T00\"),\n",
        "    np.datetime64(\"2020-01-02T00\"),\n",
        "    np.timedelta64(6, \"h\"),\n",
        ")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# ERA5 WINDOWING — *EXACTLY YOUR CODE*\n",
        "# ============================================================\n",
        "\n",
        "def _open_and_trim(path: str) -> xr.Dataset:\n",
        "    ds = xr.open_dataset(path)\n",
        "    if \"time\" in ds.dims and ds.sizes[\"time\"] > 4:\n",
        "        ds = ds.isel(time=slice(0, 4))\n",
        "    return ds\n",
        "\n",
        "\n",
        "def three_step_window(data_dir: str, center_time: str) -> xr.Dataset | None:\n",
        "    t0 = np.datetime64(center_time)\n",
        "    t_minus = t0 - np.timedelta64(6, \"h\")\n",
        "    t_plus  = t0 + np.timedelta64(6, \"h\")\n",
        "\n",
        "    needed_days = sorted({\n",
        "        np.datetime64(t_minus, \"D\"),\n",
        "        np.datetime64(t0, \"D\"),\n",
        "        np.datetime64(t_plus, \"D\"),\n",
        "    })\n",
        "\n",
        "    file_paths = [\n",
        "        os.path.join(data_dir, f\"era5_{str(d)[:10]}.nc\")\n",
        "        for d in needed_days\n",
        "    ]\n",
        "\n",
        "    if any(not os.path.exists(p) for p in file_paths):\n",
        "        return None\n",
        "\n",
        "    daily = [_open_and_trim(p) for p in file_paths]\n",
        "\n",
        "    var_time   = [v for v, da in daily[0].data_vars.items() if \"time\" in da.dims]\n",
        "    var_static = [v for v, da in daily[0].data_vars.items() if \"time\" not in da.dims]\n",
        "\n",
        "    ds_time = xr.concat([d[var_time] for d in daily], dim=\"time\").sortby(\"time\")\n",
        "    ds_static = daily[0][var_static]\n",
        "\n",
        "    ds = xr.merge([ds_time, ds_static])\n",
        "\n",
        "    target_times = np.array([t_minus, t0, t_plus], dtype=ds.time.dtype)\n",
        "    if not all(t in ds.time.values for t in target_times):\n",
        "        return None\n",
        "\n",
        "    ds = ds.sel(time=target_times)\n",
        "\n",
        "    ds_new = ds.copy()\n",
        "    for v in ds_new.data_vars:\n",
        "        if \"time\" in ds_new[v].dims:\n",
        "            ds_new[v] = ds_new[v].expand_dims(\"batch\")\n",
        "\n",
        "    for c in ds.coords:\n",
        "        if \"time\" in ds[c].dims:\n",
        "            ds_new = ds_new.assign_coords(\n",
        "                {c: ds[c].expand_dims(\"batch\")}\n",
        "            )\n",
        "\n",
        "    time_orig = ds[\"time\"]\n",
        "    t_ref = time_orig.values[0]\n",
        "    time_delta = time_orig - t_ref\n",
        "\n",
        "    ds_new = ds_new.assign_coords(time=time_delta)\n",
        "    ds_new = ds_new.assign_coords(datetime=(\"time\", time_orig.values))\n",
        "    ds_new = ds_new.assign_coords(\n",
        "        {\"datetime\": ds_new[\"datetime\"].expand_dims(\"batch\")}\n",
        "    )\n",
        "\n",
        "    return ds_new\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# LOAD GRAPHCAST + STATS — *EXACTLY YOUR CODE*\n",
        "# ============================================================\n",
        "\n",
        "gcs = storage.Client.create_anonymous_client()\n",
        "bucket = gcs.get_bucket(\"dm_graphcast\")\n",
        "prefix = \"graphcast/\"\n",
        "\n",
        "model_source = (\n",
        "    \"GraphCast - ERA5 1979-2017 - resolution 0.25 - pressure levels 37 \"\n",
        "    \"- mesh 2to6 - precipitation input and output.npz\"\n",
        ")\n",
        "\n",
        "with bucket.blob(f\"{prefix}params/{model_source}\").open(\"rb\") as f:\n",
        "    ckpt = checkpoint.load(f, graphcast.CheckPoint)\n",
        "\n",
        "model_config = ckpt.model_config\n",
        "task_config = ckpt.task_config\n",
        "params = ckpt.params\n",
        "state = {}\n",
        "\n",
        "with bucket.blob(prefix + \"stats/diffs_stddev_by_level.nc\").open(\"rb\") as f:\n",
        "    diffs_stddev_by_level = xr.load_dataset(f).compute()\n",
        "\n",
        "with bucket.blob(prefix + \"stats/mean_by_level.nc\").open(\"rb\") as f:\n",
        "    mean_by_level = xr.load_dataset(f).compute()\n",
        "\n",
        "with bucket.blob(prefix + \"stats/stddev_by_level.nc\").open(\"rb\") as f:\n",
        "    stddev_by_level = xr.load_dataset(f).compute()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# GRAPHCAST CONSTRUCTION — UNCHANGED\n",
        "# ============================================================\n",
        "\n",
        "def construct_wrapped_graphcast(model_config, task_config):\n",
        "    predictor = graphcast.GraphCast(model_config, task_config)\n",
        "    predictor = casting.Bfloat16Cast(predictor)\n",
        "    predictor = normalization.InputsAndResiduals(\n",
        "        predictor,\n",
        "        diffs_stddev_by_level=diffs_stddev_by_level,\n",
        "        mean_by_level=mean_by_level,\n",
        "        stddev_by_level=stddev_by_level,\n",
        "    )\n",
        "    predictor = autoregressive.Predictor(\n",
        "        predictor, gradient_checkpointing=True\n",
        "    )\n",
        "    return predictor\n",
        "\n",
        "\n",
        "@hk.transform_with_state\n",
        "def run_forward(model_config, task_config, inputs, targets_template, forcings):\n",
        "    predictor = construct_wrapped_graphcast(model_config, task_config)\n",
        "    return predictor(inputs, targets_template=targets_template, forcings=forcings)\n",
        "\n",
        "\n",
        "def with_configs(fn):\n",
        "    return functools.partial(fn, model_config=model_config, task_config=task_config)\n",
        "\n",
        "\n",
        "def with_params(fn):\n",
        "    return functools.partial(fn, params=params, state=state)\n",
        "\n",
        "\n",
        "def drop_state(fn):\n",
        "    return lambda **kw: fn(**kw)[0]\n",
        "\n",
        "\n",
        "run_forward_jitted = drop_state(\n",
        "    with_params(\n",
        "        jax.jit(with_configs(run_forward.apply))\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# ACTIVATION MANAGER — DISK, SUPPORTED\n",
        "# ============================================================\n",
        "\n",
        "am = get_activation_manager()\n",
        "am.__init__(\n",
        "    enabled=True,\n",
        "    save_dir=acts_dir,\n",
        "    save_steps=[2, 4, 6, 8, 10, 12, 14],\n",
        "    save_node_sets=[\"mesh_nodes\"],\n",
        "    mode=\"post_res\",\n",
        ")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# MAIN LOOP — SAME SEMANTICS AS YOUR SCRIPT\n",
        "# ============================================================\n",
        "\n",
        "t_start = time.time()\n",
        "\n",
        "for center in centers:\n",
        "    center_str = np.datetime_as_string(center, unit=\"h\")\n",
        "    print(f\"[TIME] {center_str}\")\n",
        "\n",
        "    am.set_time(center_str)\n",
        "\n",
        "    ds = three_step_window(data_dir, center_str)\n",
        "    if ds is None:\n",
        "        print(f\"[MISS] {center_str}\")\n",
        "        continue\n",
        "\n",
        "    inputs, targets, forcings = data_utils.extract_inputs_targets_forcings(\n",
        "        ds,\n",
        "        target_lead_times=slice(\"6h\", \"6h\"),\n",
        "        **dataclasses.asdict(task_config),\n",
        "    )\n",
        "\n",
        "    _ = rollout.chunked_prediction(\n",
        "        run_forward_jitted,\n",
        "        rng=jax.random.PRNGKey(0),\n",
        "        inputs=inputs,\n",
        "        targets_template=targets * np.nan,\n",
        "        forcings=forcings,\n",
        "    )\n",
        "\n",
        "    print(f\"[DONE] {center_str}\")\n",
        "\n",
        "print(f\"[ALL DONE] {time.time() - t_start:.1f}s\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "iH76JKjGjsG5",
        "outputId": "a99c3cc7-9744-4690-dff9-e297d8c130a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "iH76JKjGjsG5",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TIME] 2020-01-01T00\n",
            "[MISS] 2020-01-01T00\n",
            "[TIME] 2020-01-01T06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/graphcast/rollout.py:295: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
            "  num_target_steps = targets_template.dims[\"time\"]\n",
            "/usr/local/lib/python3.11/dist-packages/graphcast/autoregressive.py:202: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
            "  scan_length = targets_template.dims['time']\n",
            "WARNING:absl:Skipping gradient checkpointing for sequence length of 1\n",
            "/usr/local/lib/python3.11/dist-packages/graphcast/autoregressive.py:115: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
            "  num_inputs = inputs.dims['time']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DONE] 2020-01-01T06\n",
            "[TIME] 2020-01-01T12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/graphcast/rollout.py:295: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
            "  num_target_steps = targets_template.dims[\"time\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DONE] 2020-01-01T12\n",
            "[TIME] 2020-01-01T18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/graphcast/rollout.py:295: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
            "  num_target_steps = targets_template.dims[\"time\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DONE] 2020-01-01T18\n",
            "[ALL DONE] 112.2s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dhMc-wQmoKNY"
      },
      "id": "dhMc-wQmoKNY",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "runtime_attributes": {
        "runtime_version": "2025.07"
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}